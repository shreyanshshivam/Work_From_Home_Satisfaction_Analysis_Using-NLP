{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and sampling for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import collections \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78606\n"
     ]
    }
   ],
   "source": [
    "#read in raw data\n",
    "col_list = [\"text\"]\n",
    "df_all=pd.read_csv(\"Merged240520to200620.csv\")\n",
    "df = pd.read_csv(\"Merged240520to200620.csv\", usecols = col_list)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see full column content instead of truncated text\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #Millennials\\nMove to @CityOfMemphis - great place for those who #WorkFromHome\\nhttps://t.co/XTkAY3vSPX                                                                                                                                                                                               \n",
      "1    Billy Porter - Engaging Politically &amp; Creatively During COVID-19 | The Daily Social Distancing Show #digitalmarketing #workfromhome https://t.co/evEDrXTTLU                                                                                                                                       \n",
      "2    Surprising ! Real job\\nhttps://t.co/5079HFQq5x\\n#WorkFromHome                                                                                                                                                                                                                                         \n",
      "3    Exciting ! Appen job !\\nhttps://t.co/Rzxwiy1ryb\\n#appen \\n#WorkFromHome                                                                                                                                                                                                                               \n",
      "4    This article brought a smile to my face! Great to hear from fellow @BofA_News parents about the things they’re doing to keep moving forward while juggling #WFH with kids. https://t.co/6BwaDTiXt0 https://t.co/HI1MbSOZIe                                                                            \n",
      "5    If you’re struggling with your Wifi connection at home, here are 3 easy things you can do for pain-free gaming, streaming, and working.\\n#Ad @tplink #WFH #TechTips #TPLink https://t.co/IFQ5LZ4eOt                                                                                                   \n",
      "6    Easily play animations on one or more RGB LED matrix panels using a simple web-based UI with an integrated pixel editor. via @Hacksterio https://t.co/MYMJFXw0B8 #Engineer #Engineering #consultant #freelance #freelancer #freelancers #workfromhome #wfh #makers #makerspace https://t.co/JtO0Cgg1i3\n",
      "7    Earn $200 Cash Back for signing up at Freedom Credit Cards | https://t.co/1EYVeSYRTh \\n at \\n\\nhttps://t.co/BDr7WpYiss\\n\\n#income #earncash #makemoney #inspire #workfromhome #finance #investing #news #FIRE  #credit #freedom                                                                       \n",
      "8    Accomplishments by going online necessitates you to be more smart and often just like this guide here https://t.co/l7oZzZCOEa #ChangeYourLife #MakeYourOwnLane #LifeLessons #WorkFromHome https://t.co/8JLh7BHiDq                                                                                     \n",
      "9    Finding the Balance in Leadership Styles https://t.co/JAFu6FuRff via @McKinsey | #culture #workforce #WFH                                                                                                                                                                                             \n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# first x rows\n",
    "print(df[\"text\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install tweet preprocessor package to remove URLs, hashtags, mentions\n",
    "#pip install tweet-preprocessor\n",
    "\n",
    "import preprocessor as p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "['u', 'amp', 'th', 'via', 'pm', 'e', 'work', 'working', 'home', 'like', 'youre', 'must', 'gt', 'hey', 'k', 'doesnt', 'im', 'wfh', 'dont', 'june', 'many', 'day', 'time', 'week', 'make', 'back', 'may', 'going', 'today', 'thanks', 'friday', 'get', 'want', 'need', 'please', 'must', 'know', 'remote', 'office']\n"
     ]
    }
   ],
   "source": [
    "#import stoplist from file\n",
    "\n",
    "stop=pd.read_csv(\"text_stopwords.csv\")\n",
    "print(len(stop))\n",
    "#print(stop.head())\n",
    "\n",
    "stop2=stop.Stopwords.tolist()\n",
    "print(stop2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "#to add on to stop_list - used nltk as stop list is in a list. \n",
    "\n",
    "#on 15 July \n",
    "stop_list += stop2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets1 = []\n",
    "all_tweets2=[]\n",
    "all_tweets3=[]\n",
    "all_tweets4=[] #for sentiment analysis: no stemming and stopwords\n",
    "all_tweets5=[] # remove stopwords\n",
    "all_tweets6=[] #lemmitised instead of stemming\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for row in df[\"text\"]:\n",
    "    \n",
    "    # remove all urls,mentions, hashtags using twitter preprocessing p\n",
    "    tweets1 = p.clean(row)\n",
    "    all_tweets1.append(tweets1)\n",
    "\n",
    "    #remove digits,punctuations\n",
    "   \n",
    "    tweets2=re.sub(r\"\\b\\d+\\b\",\"\",tweets1)\n",
    "    all_tweets2.append(tweets2)\n",
    "    \n",
    "    \n",
    "    #tokenise \n",
    "    tweets3=list(gensim.utils.tokenize(tweets2))\n",
    "    all_tweets3.append(tweets3)\n",
    "    \n",
    "    #convert to lower case\n",
    "\n",
    "    tweets4 = [word.lower() for word in tweets3]\n",
    "    all_tweets4.append(tweets4)\n",
    "    \n",
    "    \n",
    "    #remove stopwords\n",
    "    tweets5= [word for word in tweets4 if word not in stop_list]\n",
    "    all_tweets5.append(tweets5)\n",
    "    \n",
    "    \n",
    "    #lemmatise\n",
    "    tweets6 = [lemmatizer.lemmatize(word) for word in tweets5]\n",
    "    all_tweets6.append(tweets6)\n",
    "\n",
    "    \n",
    "processed=pd.DataFrame({'sentiment':all_tweets4,'topic':all_tweets6})\n",
    "cleaned = pd.concat([df_all, processed], axis=1)\n",
    "cleaned.to_csv('Cleaned240520to200620_v2.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get word frequency distribution of raw cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('home', 17427),\n",
       " ('work', 15158),\n",
       " ('working', 10779),\n",
       " ('new', 7539),\n",
       " ('remote', 6306),\n",
       " ('amp', 6214),\n",
       " ('time', 6028),\n",
       " ('office', 5889),\n",
       " ('day', 5715),\n",
       " ('business', 5454)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert column topic into nested list (from cleaned v1 dataset)\n",
    "\n",
    "token = cleaned['topic'].tolist()\n",
    "#print(token)\n",
    "\n",
    "# List of all words across tweets - flatten list into 1 list\n",
    "all_words = list(itertools.chain(*token))\n",
    "\n",
    "# Create counter\n",
    "counts =collections.Counter(all_words)\n",
    "\n",
    "#get top 10 words - can add into stoplist\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get random tweets for subsequent classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get random 150 tweets for manual labelling - on 8 July\n",
    "\n",
    "#sample = cleaned.sample(n=150)\n",
    "#sample.to_csv('sampletest_sentiment.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54429, 48642, 49822, 26453, 46244, 2596, 43279, 68800, 35778, 65330, 42824, 2432, 56470, 69909, 68856, 28665, 54521, 4844, 5706, 62382, 42136, 3909, 60065, 23631, 49677, 78193, 3226, 56794, 51532, 23407, 41324, 9503, 76686, 60066, 806, 13521, 73756, 15933, 42456, 4643, 35079, 76755, 65687, 9302, 60762, 23788, 65983, 51561, 54472, 72700, 61018, 46159, 46459, 25924, 57552, 56462, 74715, 61359, 25551, 10621, 64036, 1860, 17004, 45554, 76836, 30509, 49654, 19663, 27710, 23780, 4699, 38724, 40993, 73701, 39831, 62385, 212, 23639, 33581, 12060, 19492, 44478, 33913, 55270, 44073, 4009, 17385, 37084, 18756, 15126, 58648, 70932, 55161, 49214, 57852, 6354, 28697, 8172, 54342, 63773, 22802, 65052, 57304, 17218, 15266, 31905, 33614, 39644, 21135, 65162, 69073, 67643, 47906, 59184, 46243, 21970, 71839, 66847, 57588, 65117, 19234, 67773, 29718, 51284, 74947, 57892, 3890, 69401, 74302, 54411, 40443, 33892, 58462, 70387, 8194, 54333, 6940, 2772, 8437, 52921, 36406, 30491, 28227, 19424, 73520, 9592, 3039, 64156, 48347, 70581]\n"
     ]
    }
   ],
   "source": [
    "#get identifiers of the sample - row_num\n",
    "col_list = [\"row_num\"]\n",
    "sample1 = pd.read_csv('sampletest_sentiment.csv', usecols=col_list)\n",
    "sample1a=sample1['row_num'].tolist()\n",
    "\n",
    "print(sample1a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get cleaned sample 1\n",
    "\n",
    "sample_train = cleaned[cleaned.row_num.isin(sample1a)]\n",
    "sample_train.to_csv('sample_train_class.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78456\n"
     ]
    }
   ],
   "source": [
    "#get 2nd sample for testing - 15 july\n",
    "\n",
    "#sample2 = cleaned[~cleaned.row_num.isin(sample1a)]\n",
    "#print(len(sample2))\n",
    "\n",
    "#sample_test = sample2.sample(n=150)\n",
    "#sample_test.to_csv('sample_test_class.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32152, 5800, 43971, 66761, 7808, 13845, 28018, 50182, 49585, 47442, 40752, 16848, 35764, 16410, 65450, 9443, 62272, 61049, 6447, 11679, 54763, 56206, 67042, 67505, 58875, 25080, 76693, 65495, 20456, 19922, 3237, 71640, 6343, 12244, 76026, 43707, 61998, 65942, 35654, 76087, 19059, 66882, 4259, 29798, 25805, 890, 26905, 18082, 51951, 12018, 19245, 71791, 17857, 33630, 5604, 9047, 26639, 77005, 28915, 24448, 38281, 44974, 26574, 67271, 18012, 20367, 27876, 43321, 18058, 35243, 54284, 30988, 55327, 56894, 31768, 67545, 78208, 44446, 70580, 57172, 47964, 23691, 69121, 19490, 29971, 13956, 13447, 15083, 28799, 12655, 11003, 61062, 8458, 75840, 56421, 2756, 41450, 56864, 76849, 75093, 47602, 9457, 45296, 17111, 28300, 77299, 63404, 20429, 53364, 42426, 29977, 48899, 32131, 31794, 7853, 16797, 71141, 47427, 41143, 33110, 50688, 25174, 6525, 13763, 29761, 35758, 66852, 27234, 74627, 47911, 44309, 21127, 35064, 13803, 36978, 62911, 60753, 29787, 40291, 38106, 26568, 58894, 49569, 68594, 58721, 37462, 47087, 55414, 20193, 32993]\n"
     ]
    }
   ],
   "source": [
    "#get identifiers of the sample - row_num\n",
    "col_list = [\"row_num\"]\n",
    "sample3 = pd.read_csv('sample_test_class.csv', usecols=col_list)\n",
    "sample3a=sample3['row_num'].tolist()\n",
    "\n",
    "print(sample3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78306\n"
     ]
    }
   ],
   "source": [
    "#get another 450 data for training - 16 July \n",
    "total_ex = sample1a + sample3a\n",
    "\n",
    "\n",
    "sample4 = cleaned[~cleaned.row_num.isin(total_ex)]\n",
    "print(len(sample4))\n",
    "\n",
    "sample4 = sample4.sample(n=450)\n",
    "sample4.to_csv('sample_train_class2.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75470, 15212, 15414, 76976, 47009, 58219, 45035, 20542, 44647, 4876, 75709, 36770, 9762, 53661, 55826, 65346, 76826, 43559, 46949, 74416, 24827, 51988, 36452, 42579, 24542, 13745, 27262, 17396, 24317, 3699, 61249, 28730, 74568, 40793, 20905, 31581, 34498, 12314, 75669, 46885, 41637, 11183, 39509, 32516, 18766, 23953, 40849, 49240, 29897, 5935, 23827, 32854, 49937, 37720, 27503, 36326, 78575, 58801, 9283, 49216, 49178, 49047, 39023, 63759, 14809, 24483, 24492, 49428, 53772, 61880, 70911, 12002, 24803, 12154, 13917, 45493, 75822, 31816, 43549, 14330, 8581, 43949, 59567, 19135, 9603, 33564, 16129, 22253, 29497, 38929, 19219, 38703, 11298, 5127, 62607, 2522, 75615, 59063, 10895, 48085, 34556, 3379, 34772, 48390, 42057, 28485, 55941, 54836, 58068, 20691, 68195, 68575, 78476, 50950, 5429, 31071, 46047, 2666, 40550, 46735, 54057, 18894, 448, 68503, 30493, 29253, 16666, 72639, 45149, 33016, 56772, 30756, 9570, 59609, 40673, 40968, 113, 77141, 57571, 24355, 23920, 74464, 35021, 52410, 27013, 27991, 68433, 38357, 30840, 78233, 28075, 53052, 49968, 45638, 38345, 17446, 52673, 53697, 10020, 16192, 22306, 10714, 10404, 16384, 68321, 33968, 71555, 59216, 21280, 38185, 7975, 70709, 49813, 31168, 65000, 39222, 17076, 77062, 16222, 20773, 3067, 16646, 74383, 12036, 62953, 13366, 16280, 4695, 8050, 54367, 26444, 71908, 48720, 37098, 54499, 6930, 35381, 41668, 62519, 40328, 14735, 58687, 27482, 46218, 8048, 18909, 64044, 72060, 35945, 44464, 9889, 70597, 46764, 58727, 78189, 46490, 15478, 6555, 22816, 543, 51061, 31678, 3275, 23740, 45154, 62822, 44148, 56231, 37811, 24023, 37582, 6533, 62125, 18997, 31763, 50437, 21224, 42082, 1599, 28783, 57361, 41103, 25911, 45761, 27383, 21043, 49663, 60213, 50506, 35969, 3899, 62609, 2066, 61874, 13701, 63499, 56157, 46992, 43059, 5522, 38522, 3496, 61364, 69335, 25600, 18143, 28893, 58332, 35377, 18658, 61248, 20949, 5869, 64542, 40575, 11147, 63952, 76303, 22563, 59686, 18853, 37229, 71810, 58662, 62560, 43360, 73221, 24572, 37696, 40766, 14855, 73686, 16211, 56874, 21561, 16101, 3724, 30514, 37783, 52311, 68831, 15124, 34235, 61509, 45454, 75635, 53721, 60627, 17405, 72192, 34130, 9086, 74341, 49746, 36943, 56332, 9476, 71576, 53805, 7898, 77452, 51181, 7198, 12263, 58210, 18683, 10791, 66549, 74862, 20219, 34477, 29688, 38578, 3809, 51731, 77523, 60564, 42940, 76854, 67549, 8974, 36415, 20293, 18021, 8891, 4548, 55427, 11415, 13571, 16051, 64059, 62666, 13228, 10557, 59879, 51006, 62853, 180, 453, 38898, 6701, 9272, 78573, 51712, 7490, 15704, 7897, 45815, 65291, 14108, 3604, 43942, 53107, 56181, 4002, 16177, 37870, 61988, 41233, 20250, 25760, 36280, 71723, 26917, 30537, 41836, 20398, 70092, 17890, 38318, 6811, 9922, 50291, 27681, 57030, 42025, 4701, 29827, 23554, 63519, 75491, 65086, 28831, 57369, 71721, 67494, 55359, 50744, 62631, 4616, 6151, 60976, 1166, 37266, 30753, 56965, 7258, 75555, 30968, 63341, 15198, 61740, 46385, 71285, 25356, 22932, 8238, 6329, 1068, 18168, 36830, 51878, 8939, 11022, 43489, 33424, 12759, 58749, 16960, 73434, 28270, 21317, 1548, 48024, 63273, 49011, 72425, 61224, 74610, 23698]\n"
     ]
    }
   ],
   "source": [
    "#get identifiers of the sample - row_num\n",
    "col_list = [\"row_num\"]\n",
    "sample4 = pd.read_csv('sample_train_class2.csv', usecols=col_list)\n",
    "sample4a=sample4['row_num'].tolist()\n",
    "\n",
    "print(sample4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77856\n"
     ]
    }
   ],
   "source": [
    "#get scoring data - i.e. n - 750\n",
    "\n",
    "total_ex = sample1a + sample3a + sample4a\n",
    "#print(total_ex)\n",
    "#print(len(total_ex))\n",
    "\n",
    "classi = cleaned[~cleaned.row_num.isin(total_ex)]\n",
    "classi.to_csv('data_classification_v2.csv', sep=',', index=False)\n",
    "\n",
    "print(len(classi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_num</th>\n",
       "      <th>day_num</th>\n",
       "      <th>row_num</th>\n",
       "      <th>userid</th>\n",
       "      <th>username</th>\n",
       "      <th>acctdesc</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>usercreatedts</th>\n",
       "      <th>tweetcreatedts</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_cursor_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.566623e+08</td>\n",
       "      <td>throckad</td>\n",
       "      <td>Breast surgical oncologist in Memphis, TN. Van...</td>\n",
       "      <td>Memphis, TN</td>\n",
       "      <td>76</td>\n",
       "      <td>427</td>\n",
       "      <td>1201</td>\n",
       "      <td>1/10/2012 16:48</td>\n",
       "      <td>24/5/2020 23:59</td>\n",
       "      <td>0</td>\n",
       "      <td>#Millennials\\nMove to @CityOfMemphis - great p...</td>\n",
       "      <td>[{'text': 'Millennials', 'indices': [0, 12]}, ...</td>\n",
       "      <td>1.264710e+18</td>\n",
       "      <td>['to', 'great', 'place', 'for', 'those', 'who']</td>\n",
       "      <td>['great', 'place']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.254020e+18</td>\n",
       "      <td>sandra_alvareze</td>\n",
       "      <td>Get cutting edge strategies and tips for peopl...</td>\n",
       "      <td>Miami</td>\n",
       "      <td>104</td>\n",
       "      <td>92</td>\n",
       "      <td>664</td>\n",
       "      <td>25/4/2020 12:04</td>\n",
       "      <td>24/5/2020 23:57</td>\n",
       "      <td>0</td>\n",
       "      <td>Billy Porter - Engaging Politically &amp;amp; Crea...</td>\n",
       "      <td>[{'text': 'digitalmarketing', 'indices': [104,...</td>\n",
       "      <td>1.264710e+18</td>\n",
       "      <td>['billy', 'porter', 'engaging', 'politically',...</td>\n",
       "      <td>['billy', 'porter', 'engaging', 'politically',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.250210e+18</td>\n",
       "      <td>healthadding</td>\n",
       "      <td>i am affiliate marketing, copy writing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1129</td>\n",
       "      <td>227</td>\n",
       "      <td>762</td>\n",
       "      <td>14/4/2020 23:48</td>\n",
       "      <td>24/5/2020 23:56</td>\n",
       "      <td>0</td>\n",
       "      <td>Surprising ! Real job\\nhttps://t.co/5079HFQq5x...</td>\n",
       "      <td>[{'text': 'WorkFromHome', 'indices': [46, 59]}]</td>\n",
       "      <td>1.264710e+18</td>\n",
       "      <td>['surprising', 'real', 'job']</td>\n",
       "      <td>['surprising', 'real', 'job']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.250210e+18</td>\n",
       "      <td>healthadding</td>\n",
       "      <td>i am affiliate marketing, copy writing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1129</td>\n",
       "      <td>227</td>\n",
       "      <td>762</td>\n",
       "      <td>14/4/2020 23:48</td>\n",
       "      <td>24/5/2020 23:56</td>\n",
       "      <td>0</td>\n",
       "      <td>Exciting ! Appen job !\\nhttps://t.co/Rzxwiy1ry...</td>\n",
       "      <td>[{'text': 'appen', 'indices': [47, 53]}, {'tex...</td>\n",
       "      <td>1.264710e+18</td>\n",
       "      <td>['exciting', 'appen', 'job']</td>\n",
       "      <td>['exciting', 'appen', 'job']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.347212e+09</td>\n",
       "      <td>parkhurstheidi</td>\n",
       "      <td>Iowa Market President at Bank of America. Resi...</td>\n",
       "      <td>Coal Valley, IL</td>\n",
       "      <td>1844</td>\n",
       "      <td>710</td>\n",
       "      <td>3156</td>\n",
       "      <td>24/11/2015 17:38</td>\n",
       "      <td>24/5/2020 23:56</td>\n",
       "      <td>0</td>\n",
       "      <td>This article brought a smile to my face! Great...</td>\n",
       "      <td>[{'text': 'WFH', 'indices': [155, 159]}]</td>\n",
       "      <td>1.264710e+18</td>\n",
       "      <td>['this', 'article', 'brought', 'a', 'smile', '...</td>\n",
       "      <td>['article', 'brought', 'smile', 'face', 'great...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week_num  day_num  row_num        userid         username  \\\n",
       "0         1        1        1  8.566623e+08         throckad   \n",
       "1         1        1        2  1.254020e+18  sandra_alvareze   \n",
       "2         1        1        3  1.250210e+18     healthadding   \n",
       "3         1        1        4  1.250210e+18     healthadding   \n",
       "4         1        1        5  4.347212e+09   parkhurstheidi   \n",
       "\n",
       "                                            acctdesc         location  \\\n",
       "0  Breast surgical oncologist in Memphis, TN. Van...      Memphis, TN   \n",
       "1  Get cutting edge strategies and tips for peopl...            Miami   \n",
       "2             i am affiliate marketing, copy writing              NaN   \n",
       "3             i am affiliate marketing, copy writing              NaN   \n",
       "4  Iowa Market President at Bank of America. Resi...  Coal Valley, IL   \n",
       "\n",
       "   following  followers  totaltweets     usercreatedts   tweetcreatedts  \\\n",
       "0         76        427         1201   1/10/2012 16:48  24/5/2020 23:59   \n",
       "1        104         92          664   25/4/2020 12:04  24/5/2020 23:57   \n",
       "2       1129        227          762   14/4/2020 23:48  24/5/2020 23:56   \n",
       "3       1129        227          762   14/4/2020 23:48  24/5/2020 23:56   \n",
       "4       1844        710         3156  24/11/2015 17:38  24/5/2020 23:56   \n",
       "\n",
       "   retweetcount                                               text  \\\n",
       "0             0  #Millennials\\nMove to @CityOfMemphis - great p...   \n",
       "1             0  Billy Porter - Engaging Politically &amp; Crea...   \n",
       "2             0  Surprising ! Real job\\nhttps://t.co/5079HFQq5x...   \n",
       "3             0  Exciting ! Appen job !\\nhttps://t.co/Rzxwiy1ry...   \n",
       "4             0  This article brought a smile to my face! Great...   \n",
       "\n",
       "                                            hashtags  tweet_cursor_id  \\\n",
       "0  [{'text': 'Millennials', 'indices': [0, 12]}, ...     1.264710e+18   \n",
       "1  [{'text': 'digitalmarketing', 'indices': [104,...     1.264710e+18   \n",
       "2    [{'text': 'WorkFromHome', 'indices': [46, 59]}]     1.264710e+18   \n",
       "3  [{'text': 'appen', 'indices': [47, 53]}, {'tex...     1.264710e+18   \n",
       "4           [{'text': 'WFH', 'indices': [155, 159]}]     1.264710e+18   \n",
       "\n",
       "                                           sentiment  \\\n",
       "0    ['to', 'great', 'place', 'for', 'those', 'who']   \n",
       "1  ['billy', 'porter', 'engaging', 'politically',...   \n",
       "2                      ['surprising', 'real', 'job']   \n",
       "3                       ['exciting', 'appen', 'job']   \n",
       "4  ['this', 'article', 'brought', 'a', 'smile', '...   \n",
       "\n",
       "                                               topic  \n",
       "0                                 ['great', 'place']  \n",
       "1  ['billy', 'porter', 'engaging', 'politically',...  \n",
       "2                      ['surprising', 'real', 'job']  \n",
       "3                       ['exciting', 'appen', 'job']  \n",
       "4  ['article', 'brought', 'smile', 'face', 'great...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned=pd.read_csv(\"Cleaned240520to200620_v2.csv\")\n",
    "\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
