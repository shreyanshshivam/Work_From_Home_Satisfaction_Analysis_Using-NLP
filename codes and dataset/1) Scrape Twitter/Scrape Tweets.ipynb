{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use config file with your API authentication details\n",
    "from twitter_config import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scrape\n",
    "\n",
    "def scraptweets(search_words, date_until, date_since, numTweets, numRuns):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['userid','username', 'acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'text', 'hashtags','tweet_cursor_id']\n",
    "                                )\n",
    "    program_start = time.time()\n",
    "    #maxid helps as a variable to prevent duplicated tweets when we run the function through diff. runs. \n",
    "    maxid=None \n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet. estimated s'pore geocode\n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", max_id=maxid, \n",
    "                               until=date_until,since=date_since, tweet_mode='extended').items(numTweets)\n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "       \n",
    "     \n",
    "        noTweets = 0\n",
    "        for tweet in tweet_list:\n",
    "            # Pull the values\n",
    "                userid=tweet.user.id\n",
    "                username = tweet.user.screen_name\n",
    "                acctdesc = tweet.user.description\n",
    "                location = tweet.user.location\n",
    "                following = tweet.user.friends_count\n",
    "                followers = tweet.user.followers_count\n",
    "                totaltweets = tweet.user.statuses_count\n",
    "                usercreatedts = tweet.user.created_at\n",
    "                tweetcreatedts = tweet.created_at\n",
    "                retweetcount = tweet.retweet_count\n",
    "                hashtags = tweet.entities['hashtags']\n",
    "                temp_max_id=tweet.id\n",
    "                text = tweet.full_text\n",
    "                \n",
    "                ith_tweet = [userid,username, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags,temp_max_id]\n",
    "                # Append to dataframe - db_tweets\n",
    "                db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "                # increase counter - noTweets  \n",
    "                noTweets += 1\n",
    "                \n",
    "        #find the max_id to feed into the arg for subsequent loops\n",
    "        try:\n",
    "            maxid= (tweet_list[-1].id - 1)\n",
    "        except: \n",
    "            print(\"max tweets crawled\")\n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "        \n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "        \n",
    "        time.sleep(930) #15 minute sleep time to keep within api limit\n",
    "    # Once all runs have completed, save them to a single csv file:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + to_csv_timestamp + '_wfh_tweets.csv'\n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the code below, selected search words are: \n",
    "- #workfromhome \n",
    "- #wfh \n",
    "- #LoveWhereverYouWork\n",
    "- #AnotherDayAtTheHomeOffice\n",
    "- #HomeSweetOffice\n",
    "- #MyHomeOffice\n",
    "- #SigningOnFromHome\n",
    "- #MyWFHRoutine\n",
    "- #SigningOnInSweatpants\n",
    "- #LetsWorkFromHome\n",
    "- #WorkFromWherever\n",
    "- #WhenYouWorkFromHome\n",
    "<br>\n",
    "retweets are filtered out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 1 is 1386\n",
      "time take for 1 run to complete is 2.4 mins\n",
      "max tweets crawled\n",
      "no. of tweets scraped for run 2 is 0\n",
      "time take for 2 run to complete is 0.01 mins\n",
      "Scraping has completed!\n",
      "Total time taken to scrap is 33.416666666666664 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Initialise these variables: to crawl only one day tweet (T), date_until = T+1 and date_since=T. \n",
    "search_words = \"#workfromhome OR #wfh OR #LoveWhereverYouWork OR #AnotherDayAtTheHomeOffice OR #HomeSweetOffice OR #MyHomeOffice OR #SigningOnFromHome OR #MyWFHRoutine OR #SigningOnInSweatpants OR #LetsWorkFromHome OR #WorkFromWherever OR #WhenYouWorkFromHome -filter:retweets \"\n",
    "\n",
    "#scrape tweets for 28 june\n",
    "date_until = \"2020-06-29\"\n",
    "date_since=\"2020-06-28\"\n",
    "numTweets = 2500\n",
    "numRuns = 2\n",
    "# Call the function scraptweets\n",
    "scraptweets(search_words, date_until, date_since, numTweets, numRuns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
